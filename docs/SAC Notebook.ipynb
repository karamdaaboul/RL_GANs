{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl_gans.model.model import SAC_Model\n",
    "from rl_gans.algos.sac import SAC\n",
    "from rl_gans.utils.argument import parse_args\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(action_repeat=4, actor_beta=0.9, actor_log_std_max=2, actor_log_std_min=-10, actor_lr=0.001, actor_update_freq=2, agent='sac', agent_image_size=84, alpha_beta=0.5, alpha_lr=0.0001, batch_size=128, critic_beta=0.9, critic_encoder_tau=0.05, critic_lr=0.001, critic_target_update_freq=2, critic_tau=0.01, detach_encoder=False, discount=0.99, discriminator_beta=0.5, discriminator_lr=0.0001, discriminator_update_freq=2, domain_name='cheetah', encoder_feature_dim=50, env_image_size=84, eval_freq=2000, frame_stack=3, generator_beta=0.5, generator_lr=0.0001, generator_update_freq=1, hidden_dim=1024, image_pad=None, init_steps=1000, init_temperature=0.1, log_interval=25, num_eval_episodes=10, num_filters=32, num_layers=4, num_train_steps=10000, numupdates=100, replay_buffer_capacity=100000, save_buffer=True, save_model=False, save_tb=True, save_video=False, seed=1, tag='', task_name='run', work_dir='.')\n"
     ]
    }
   ],
   "source": [
    "args = parse_args()\n",
    "args.agent = \"sac\"\n",
    "args.env_image_size = 84\n",
    "args.agent_image_size = 84\n",
    "device = 'cuda'\n",
    "args.save_tb = True\n",
    "#Modify args for test run\n",
    "\n",
    "args.num_train_steps = 10000\n",
    "args.eval_freq = 2000\n",
    "args.init_steps= 1000\n",
    "args.numupdates= 100\n",
    "args.save_video = False\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msafe-transfer-learning-in-changing-environments\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.17 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/daaboul/workspaces/Teaching/RL_GANs/docs/wandb/run-20220601_180553-1ev31x3s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/karamdaaboul/SAC%20Test/runs/1ev31x3s\" target=\"_blank\">chocolate-tree-8</a></strong> to <a href=\"https://wandb.ai/karamdaaboul/SAC%20Test\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "            project=\"SAC Test\",\n",
    "            entity=\"karamdaaboul\",\n",
    "            name=None,\n",
    "            config=args,\n",
    "            #sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics\n",
    "            monitor_gym=True,  # auto-upload the videos of agents playing the game\n",
    "            save_code=True,  # optional\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.tensorboard.patch(root_logdir=f'{args.work_dir}/tb', pytorch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl_gans.memory import ReplayBufferStorage\n",
    "from rl_gans.memory.replay_buffer import make_replay_buffer\n",
    "from rl_gans.utils.misc import set_seed_everywhere, make_dir, VideoRecorder, eval_mode\n",
    "from rl_gans.utils.logger import Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./cheetah-run-06-01-im84-b128-s1-sac\n"
     ]
    }
   ],
   "source": [
    "ts = time.strftime(\"%m-%d\", time.gmtime())\n",
    "env_name = args.domain_name + '-' + args.task_name\n",
    "exp_name = env_name + '-' + ts + '-im' + str(args.env_image_size) +'-b'  \\\n",
    "+ str(args.batch_size) + '-s' + str(args.seed)  + '-' + args.agent\n",
    "args.work_dir = args.work_dir + '/'  + exp_name\n",
    "make_dir(args.work_dir)\n",
    "video_dir = make_dir(os.path.join(args.work_dir, 'video'))\n",
    "model_dir = make_dir(os.path.join(args.work_dir, 'model'))\n",
    "\n",
    "os.environ['MKL_SERVICE_FORCE_INTEL'] = '1'\n",
    "os.environ['MUJOCO_GL'] = 'egl'\n",
    "video = VideoRecorder(dir_name = video_dir if args.save_video else None)\n",
    "\n",
    "print(args.work_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(args.work_dir, 'args.json'), 'w') as f:\n",
    "    json.dump(vars(args), f, sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "Creating window glfw\n"
     ]
    }
   ],
   "source": [
    "# prepare env\n",
    "from rl_gans.wrappers.pixel_observation_wrapper import PixelObservation\n",
    "env = gym.make(\"HalfCheetah-v3\")\n",
    "print(env._max_episode_steps)\n",
    "env = PixelObservation(env,observation_size= args.env_image_size ,normalize=False)\n",
    "\n",
    "eval_env = gym.make(\"HalfCheetah-v3\")\n",
    "eval_env = PixelObservation(env,observation_size= args.env_image_size ,normalize=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_shape: (6,)\n",
      "observation_shape: Agent (3, 84, 84), Environment (3, 84, 84)\n"
     ]
    }
   ],
   "source": [
    "action_shape = env.action_space.shape\n",
    "args.env_image_size = 84\n",
    "agent_obs_shape = (3, args.agent_image_size, args.agent_image_size)\n",
    "env_obs_shape = (3, args.env_image_size, args.env_image_size)\n",
    "\n",
    "action_shape = env.action_space.shape\n",
    "print(f\"action_shape: {action_shape}\")\n",
    "observation_shape = env.observation_space.shape\n",
    "print(f\"observation_shape: Agent {agent_obs_shape}, Environment {env_obs_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the sac model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl_gans.model import SAC_Model\n",
    "\n",
    "sac_model = SAC_Model(obs_shape = env_obs_shape,\n",
    "                     action_shape        = action_shape,       \n",
    "                     hidden_dim          = args.hidden_dim,\n",
    "                     encoder_feature_dim = args.encoder_feature_dim,\n",
    "                     log_std_min         = args.actor_log_std_min,\n",
    "                     log_std_max         = args.actor_log_std_max,\n",
    "                     num_layers          = args.num_layers, \n",
    "                     num_filters         = args.num_filters, \n",
    "                     device  = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl_gans.algos.sac import SAC\n",
    "\n",
    "args.detach_encoder\n",
    " \n",
    "agent = SAC(model       = sac_model, \n",
    "            device      = device, \n",
    "            action_shape=action_shape,  \n",
    "            args        = args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, agent, video, num_episodes, L, step, tag=None):\n",
    "    episode_rewards = []\n",
    "    for i in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        video.init(enabled=(i==0))\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        while not done:\n",
    "            with eval_mode(agent):\n",
    "                action = agent.select_action(obs)\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            video.record(env)\n",
    "            episode_reward += reward\n",
    "\n",
    "        if L is not None:\n",
    "            video.save(f'{step}.mp4')\n",
    "            L.log(f'eval/episode_reward', episode_reward, step)\n",
    "        episode_rewards.append(episode_reward)\n",
    "    \n",
    "    return np.mean(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<rl_gans.algos.sac.SAC object at 0x7ff05b543250>\n",
      "SAC_Model(\n",
      "  (actor): Actor(\n",
      "    (encoder): Encoder(\n",
      "      (cnn): SharedCNN(\n",
      "        (layers): Sequential(\n",
      "          (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2))\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (3): ReLU()\n",
      "          (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (5): ReLU()\n",
      "          (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (7): Flatten()\n",
      "        )\n",
      "      )\n",
      "      (projection): RLProjection(\n",
      "        (projection): Sequential(\n",
      "          (0): Linear(in_features=39200, out_features=50, bias=True)\n",
      "          (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
      "          (2): Tanh()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=50, out_features=1024, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=1024, out_features=12, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (critic): Critic(\n",
      "    (encoder): Encoder(\n",
      "      (cnn): SharedCNN(\n",
      "        (layers): Sequential(\n",
      "          (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2))\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (3): ReLU()\n",
      "          (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (5): ReLU()\n",
      "          (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (7): Flatten()\n",
      "        )\n",
      "      )\n",
      "      (projection): RLProjection(\n",
      "        (projection): Sequential(\n",
      "          (0): Linear(in_features=39200, out_features=50, bias=True)\n",
      "          (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
      "          (2): Tanh()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (Q1): QFunction(\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=56, out_features=1024, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=1024, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (Q2): QFunction(\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=56, out_features=1024, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=1024, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (critic_target): Critic(\n",
      "    (encoder): Encoder(\n",
      "      (cnn): SharedCNN(\n",
      "        (layers): Sequential(\n",
      "          (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2))\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (3): ReLU()\n",
      "          (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (5): ReLU()\n",
      "          (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (7): Flatten()\n",
      "        )\n",
      "      )\n",
      "      (projection): RLProjection(\n",
      "        (projection): Sequential(\n",
      "          (0): Linear(in_features=39200, out_features=50, bias=True)\n",
      "          (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
      "          (2): Tanh()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (Q1): QFunction(\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=56, out_features=1024, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=1024, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (Q2): QFunction(\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=56, out_features=1024, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=1024, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(agent)\n",
    "print(sac_model)\n",
    "print(args.init_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.image_pad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./cheetah-run-06-01-im84-b128-s1-sac'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.work_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Found logdirectory outside of given root_logdir, dropping given root_logdir for eventfile in ./cheetah-run-06-01-im84-b128-s1-sac/tb\n"
     ]
    }
   ],
   "source": [
    "# run\n",
    "replay_storage = ReplayBufferStorage(Path(args.work_dir) / 'buffer')\n",
    "replay_buffer = None\n",
    "\n",
    "L = Logger(args.work_dir, use_tb=args.save_tb, config=args.agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1\n",
      "| \u001b[33mtrain\u001b[0m | E: 1 | S: 1000 | D: 1.0 s | R: -243.7209 | BR: 0.0000 | A_LOSS: 0.0000 | CR_LOSS: 0.0000\n",
      "episode 2\n",
      "<bound method ReplayBuffer.sample of <rl_gans.memory.replay_buffer.ReplayBuffer object at 0x7ff09891e490>>\n",
      "evaluation\n",
      "| \u001b[33mtrain\u001b[0m | E: 2 | S: 2000 | D: 746.0 s | R: -147.5789 | BR: -0.2456 | A_LOSS: 0.0000 | CR_LOSS: 0.3294\n",
      "| \u001b[32meval\u001b[0m | S: 2000 | ER: -153.7922\n",
      "episode 3\n",
      "| \u001b[33mtrain\u001b[0m | E: 3 | S: 3000 | D: 725.5 s | R: -180.6900 | BR: -0.1907 | A_LOSS: 0.0000 | CR_LOSS: 0.2758\n",
      "episode 4\n",
      "evaluation\n",
      "| \u001b[33mtrain\u001b[0m | E: 4 | S: 4000 | D: 730.1 s | R: -134.8358 | BR: -0.1857 | A_LOSS: 0.0000 | CR_LOSS: 0.1779\n",
      "| \u001b[32meval\u001b[0m | S: 4000 | ER: -223.0792\n",
      "episode 5\n",
      "| \u001b[33mtrain\u001b[0m | E: 5 | S: 5000 | D: 723.9 s | R: -187.0819 | BR: -0.1731 | A_LOSS: 0.0000 | CR_LOSS: 0.1911\n",
      "episode 6\n",
      "evaluation\n",
      "| \u001b[33mtrain\u001b[0m | E: 6 | S: 6000 | D: 731.3 s | R: -247.3464 | BR: -0.1772 | A_LOSS: 0.0000 | CR_LOSS: 0.1616\n",
      "| \u001b[32meval\u001b[0m | S: 6000 | ER: -19.6008\n",
      "episode 7\n",
      "| \u001b[33mtrain\u001b[0m | E: 7 | S: 7000 | D: 725.9 s | R: -83.8461 | BR: -0.1928 | A_LOSS: 0.0000 | CR_LOSS: 0.1346\n",
      "episode 8\n",
      "evaluation\n",
      "| \u001b[33mtrain\u001b[0m | E: 8 | S: 8000 | D: 730.8 s | R: -137.1913 | BR: -0.1814 | A_LOSS: 0.0000 | CR_LOSS: 0.1271\n",
      "| \u001b[32meval\u001b[0m | S: 8000 | ER: -185.1978\n",
      "episode 9\n",
      "| \u001b[33mtrain\u001b[0m | E: 9 | S: 9000 | D: 725.4 s | R: -176.8477 | BR: -0.1683 | A_LOSS: 0.0000 | CR_LOSS: 0.1258\n",
      "episode 10\n",
      "evaluation\n",
      "| \u001b[33mtrain\u001b[0m | E: 10 | S: 10000 | D: 731.5 s | R: -189.7400 | BR: -0.1710 | A_LOSS: 0.0000 | CR_LOSS: 0.1066\n",
      "| \u001b[32meval\u001b[0m | S: 10000 | ER: -187.3748\n",
      "episode 11\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.455 MB of 0.455 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/episode</td><td>▁▃▅▆█</td></tr><tr><td>eval/episode_reward</td><td>▄▁█▂▃</td></tr><tr><td>global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/batch_reward</td><td>▃▂▅▁▁▄▆▆▆▅█▄▄▄▆▆▅▄▆█▄▅▆▄▇▆▆▄▅▆▆▄▇▄▆▃▄▄▅▅</td></tr><tr><td>train/duration</td><td>▁█████████</td></tr><tr><td>train/episode</td><td>▁▂▂▃▄▅▅▆▇▇█</td></tr><tr><td>train/episode_reward</td><td>▁▅▄▆▄▁█▆▄▃</td></tr><tr><td>train_actor/entropy</td><td>██▆▆▆▅▃▃▄▄▄▄▃▄▄▃▃▄▃▃▃▃▃▃▃▃▃▂▁▂▁▁▂▁▁▁▁▂▂▂</td></tr><tr><td>train_actor/loss</td><td>██▇▇▆▇▆▆▅▅▅▄▄▃▃▂▂▂▂▂▂▂▂▁▁▂▁▂▁▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_actor/target_entropy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_alpha/loss</td><td>▇█▆▅▅▆▄▄▃▄▄▂▂▁▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>train_alpha/value</td><td>██▇▇▆▆▆▆▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_critic/loss</td><td>▄▂▁▁▁█▂▂▂▄▂▂▂▃▃▂▃▃▄▂▂▂▃▃▁▂▂▂▁▁▁▅▂▃▁▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/episode</td><td>10.0</td></tr><tr><td>eval/episode_reward</td><td>-186.19005</td></tr><tr><td>global_step</td><td>10000</td></tr><tr><td>train/batch_reward</td><td>-0.0902</td></tr><tr><td>train/duration</td><td>731.51984</td></tr><tr><td>train/episode</td><td>11.0</td></tr><tr><td>train/episode_reward</td><td>-189.74002</td></tr><tr><td>train_actor/entropy</td><td>2.50255</td></tr><tr><td>train_actor/loss</td><td>-35.97055</td></tr><tr><td>train_actor/target_entropy</td><td>-6.0</td></tr><tr><td>train_alpha/loss</td><td>-0.00997</td></tr><tr><td>train_alpha/value</td><td>0.01995</td></tr><tr><td>train_critic/loss</td><td>0.07637</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">chocolate-tree-8</strong>: <a href=\"https://wandb.ai/karamdaaboul/SAC%20Test/runs/1ev31x3s\" target=\"_blank\">https://wandb.ai/karamdaaboul/SAC%20Test/runs/1ev31x3s</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220601_180553-1ev31x3s/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "episode, episode_reward, done, info = 0, 0, True, {}\n",
    "start_time = time.time()\n",
    "\n",
    "for step in range(args.num_train_steps+1):\n",
    "    # evaluate agent periodically\n",
    "\n",
    "    if step > 0 and step % args.eval_freq == 0:\n",
    "        print(\"evaluation\")\n",
    "        L.log('eval/episode', episode, step)\n",
    "        with torch.no_grad():\n",
    "            #evaluate(eval_env, agent, video, args.num_eval_episodes, L, step)\n",
    "            evaluate(eval_env, agent, video, 3, L, step)\n",
    "        if args.save_model:\n",
    "            agent.save_model(model_dir, step)\n",
    "\n",
    "    if done:\n",
    "        if step > 0:\n",
    "            replay_storage.add(obs, None, None, True)  # add the last observation for each episode\n",
    "            if step % args.log_interval == 0:\n",
    "                L.log('train/episode_reward', episode_reward, step)\n",
    "                L.log('train/duration', time.time() - start_time, step)\n",
    "                L.dump(step)\n",
    "            start_time = time.time()\n",
    "\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        episode_step = 0\n",
    "        episode += 1\n",
    "        print(\"episode\", episode)\n",
    "        if step % args.log_interval == 0:\n",
    "            L.log('train/episode', episode, step)\n",
    "\n",
    "    # sample action for data collection\n",
    "    if step < args.init_steps:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        with eval_mode(agent):\n",
    "            action = agent.sample_action(obs)\n",
    "\n",
    "    # run training update\n",
    "    if step >= args.init_steps:\n",
    "        if replay_buffer is None:\n",
    "            replay_buffer = make_replay_buffer(replay_dir=Path(args.work_dir) / 'buffer',\n",
    "                                               replay_type=\"Normal\",\n",
    "                                               max_size=args.replay_buffer_capacity,\n",
    "                                               batch_size=args.batch_size,\n",
    "                                               num_workers=1,\n",
    "                                               save_snapshot=False,\n",
    "                                               nstep=1,\n",
    "                                               discount=args.discount,\n",
    "                                               obs_shape=env_obs_shape,\n",
    "                                               device=device,\n",
    "                                               image_size=args.agent_image_size,\n",
    "                                               image_pad=args.image_pad)\n",
    "            print(replay_buffer.sample)\n",
    "\n",
    "\n",
    "        num_updates = 1 if step > args.init_steps else args.init_steps\n",
    "        for _ in range(5):\n",
    "            agent.update(replay_buffer, L, step)\n",
    "\n",
    "    next_obs, reward, done, info = env.step(action)\n",
    "\n",
    "    # allow infinit bootstrap\n",
    "    done_bool = 0 if episode_step + 1 == 1000 else float(done)\n",
    "    episode_reward += reward\n",
    "    replay_storage.add(obs, action, reward, done_bool)    \n",
    "\n",
    "    obs = next_obs\n",
    "    episode_step += 1       \n",
    "\n",
    "if run != None:\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args.eval_freq: 2000\n",
      "args.init_steps: 1000\n",
      "args.num_train_steps+1: 10001\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "print(f\"args.eval_freq: {args.eval_freq}\")\n",
    "print(f\"args.init_steps: {args.init_steps}\")\n",
    "print(f\"args.num_train_steps+1: {args.num_train_steps+1}\")\n",
    "print(episode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_wmec",
   "language": "python",
   "name": "venv_wmec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
